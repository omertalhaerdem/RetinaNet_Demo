{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q keras-retinanet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Pretrained Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Import the urlretrieve function from the urllib.request module and the os module.\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "\n",
    "# Define the name of the pretrained model, the backbone name used in the model, and the base URL for downloading the model.\n",
    "PRETRAINED_MODEL_NAME = \"resnet50_coco_best_v2.1.0.h5\"\n",
    "BACKBONE_NAME = 'resnet50'\n",
    "PRETRAINED_BASE_URL = (\n",
    "    \"https://github.com/fizyr/keras-retinanet/\"\n",
    "    \"releases/download/0.5.1/\")\n",
    "\n",
    "# If the pretrained model file does not exist:\n",
    "if not os.path.exists(PRETRAINED_MODEL_NAME):\n",
    "    # Construct the full URL of the model.\n",
    "    model_url = PRETRAINED_BASE_URL + PRETRAINED_MODEL_NAME\n",
    "    \n",
    "    # Display a message indicating that the download process is starting.\n",
    "    print(f\"Downloading {model_url}...\")\n",
    "    \n",
    "    # Download the model using the urlretrieve function and save it with the same name.\n",
    "    urlretrieve(model_url, PRETRAINED_MODEL_NAME)\n",
    "    \n",
    "    # Display a message indicating that the download process is complete.\n",
    "    print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\waspote\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\waspote\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\waspote\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\waspote\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Import the 'models' module from the 'keras_retinanet' package.\n",
    "from keras_retinanet import models\n",
    "\n",
    "# Load a pre-trained RetinaNet model using the specified model file name and backbone name.\n",
    "model = models.load_model(PRETRAINED_MODEL_NAME, backbone_name=BACKBONE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Objects (Location and Classes) in Test Images\n",
    "\n",
    "We need to define a label to names mapping for visualization purposes: those labels match the classes from the COCO dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_names = {\n",
    "    0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane',\n",
    "    5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light',\n",
    "    10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench',\n",
    "    14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow',\n",
    "    20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack',\n",
    "    25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee',\n",
    "    30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite',\n",
    "    34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard',\n",
    "    37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass',\n",
    "    41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl',\n",
    "    46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli',\n",
    "    51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake',\n",
    "    56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed',\n",
    "    60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse',\n",
    "    65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave',\n",
    "    69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book',\n",
    "    74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear',\n",
    "    78: 'hair drier', 79: 'toothbrush'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (800, 1067, 3), dtype: float32, range: (-123.68, 255)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary functions and modules from keras_retinanet.utils\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "from keras_retinanet.utils.colors import label_color\n",
    "\n",
    "# Import additional libraries for visualization and timing\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define a function for object detection and visualization\n",
    "def detect_and_visualize(image_bgr):\n",
    "    # Create a copy of the input image for visualization and convert it to RGB color format\n",
    "    draw = image_bgr.copy()\n",
    "    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Create a copy of the input image for processing\n",
    "    image_input = image_bgr.copy()\n",
    "    \n",
    "    # Preprocess the input image using functions from keras_retinanet.utils.image\n",
    "    image_input = preprocess_image(image_input)\n",
    "    \n",
    "    # Resize the preprocessed image and obtain the scale factor\n",
    "    image_input, scale = resize_image(image_input)\n",
    "    \n",
    "    # Display information about the processed image (shape, dtype, and value range)\n",
    "    print(f\"shape: {image_input.shape}, dtype: {image_input.dtype}, \"\n",
    "          f\"range: {(image_input.min(), image.max())}\")\n",
    "\n",
    "    # Record the start time for processing\n",
    "    start = time.time()\n",
    "    \n",
    "    # Perform object detection on the preprocessed image using the loaded model\n",
    "    boxes, scores, labels = model.predict_on_batch(\n",
    "        np.expand_dims(image_input, axis=0))\n",
    "    \n",
    "    # Display the processing time\n",
    "    print(f\"processing time: {time.time() - start:.1f}s\")\n",
    "\n",
    "    # Adjust the bounding box coordinates based on the scale factor\n",
    "    boxes /= scale\n",
    "\n",
    "    # Iterate through detected objects and visualize them on the image\n",
    "    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "        # If the detection score is below a threshold (0.4), skip further processing\n",
    "        if score < 0.4:\n",
    "            break\n",
    "\n",
    "        # Assign a unique color to each object class\n",
    "        color = label_color(label)\n",
    "\n",
    "        # Convert bounding box coordinates to integers\n",
    "        b = box.astype(int)\n",
    "        \n",
    "        # Draw the bounding box on the visualization image\n",
    "        draw_box(draw, b, color=color)\n",
    "\n",
    "        # Create a caption with object label and detection score\n",
    "        caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "        print(caption)\n",
    "        \n",
    "        # Draw the caption on the visualization image\n",
    "        draw_caption(draw, b, caption)\n",
    "\n",
    "    # Display the visualization image using matplotlib\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(draw)\n",
    "\n",
    "# Example usage: Read an image from file, perform detection, and visualize the results\n",
    "image = read_image_bgr('webcam_shot.jpeg')\n",
    "detect_and_visualize(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World Data\n",
    "\n",
    "Let's play with the laptop webcam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import cv2\n",
    "from keras_retinanet.utils.image import read_image_bgr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to grab a snapshot from a camera\n",
    "def camera_grab(camera_id=0, fallback_filename='webcam_shot.jpeg'):\n",
    "    # Open a connection to the camera with the specified camera_id\n",
    "    camera = cv2.VideoCapture(camera_id)\n",
    "    try:\n",
    "        # Take 10 consecutive snapshots to allow the camera to tune itself\n",
    "        # and hope that the contrast and lighting of the last snapshot are good enough.\n",
    "        for i in range(10):\n",
    "            snapshot_ok, image = camera.read()\n",
    "        # If snapshot is not successful, print a warning and use a fallback image\n",
    "        if not snapshot_ok:\n",
    "            print(\"WARNING: could not access camera\")\n",
    "            # If a fallback filename is provided, read an image from that file\n",
    "            if fallback_filename:\n",
    "                image = read_image_bgr(fallback_filename)\n",
    "    finally:\n",
    "        # Release the camera connection\n",
    "        camera.release()\n",
    "    \n",
    "    # Return the captured image\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture an image from the specified camera (camera_id=0) using the camera_grab function\n",
    "image = camera_grab(camera_id=0)\n",
    "\n",
    "# Display the captured image using matplotlib\n",
    "plt.figure(figsize=(8, 8))\n",
    "# Convert the image from BGR to RGB color format for proper display\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Do not show axis ticks\n",
    "plt.show()  # Display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the detect_and_visualize function to perform object detection and visualization on the captured image\n",
    "detect_and_visualize(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
